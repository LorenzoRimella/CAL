{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 09:57:05.903870: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-18 09:57:06.021831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-18 09:57:06.021956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-18 09:57:06.022433: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-18 09:57:06.085605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-18 09:57:08.228213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Set font properties for ticks\n",
    "plt.rcParams['xtick.labelsize'] = 12  # Font size of x-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Font size of y-axis tick labels\n",
    "\n",
    "# Set font properties for labels\n",
    "plt.rcParams['axes.labelsize'] = 15  # Font size of axis labels\n",
    "\n",
    "# Set font properties for title\n",
    "plt.rcParams['axes.titlesize'] = 17  # Font size of titlea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_ellipse(cov, centroid, ax, n_std=2.0, facecolor='none', **kwargs):\n",
    "\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = centroid[0]\n",
    "\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = centroid[1]\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"Scripts/\")\n",
    "%run -i model.py\n",
    "%run -i CAL.py\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an individual-based SIS model with individual-specific covariates $w_n = [l_n, c_n]$ where $l_n$ is the location in space of each individual and $c_n = [1 \\quad \\xi_n]^\\top$, with $\\xi_n$ standard Gaussian random variable. About $l_n$ each individual is uniformly assigned to one of $10$ cities, their location is then drawn from a bivariate Gaussian with mean and variance-covariance matrix that is specific for the considered city.\n",
    "\n",
    "Then consider the following individual-specific initial distribution, transition kernel and emission distribution:\n",
    "$$\n",
    "    p_0(w_n) \n",
    "    = \n",
    "    \\begin{bmatrix} \n",
    "    1-p_{0}\\\\ p_{0} \n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{\\eta}_{n,t-1}\n",
    "    =\n",
    "    \\frac{1}{N} \\sum_{k \\in [N]} \n",
    "    \\begin{bmatrix}\n",
    "    0\\\\\n",
    "     \\frac{\\exp\\left\\{-\\frac{\\|l_n-l_{k}\\|^2}{2\\phi}\\right\\}}{\\sqrt{2 \\pi \\phi}} \n",
    "    \\end{bmatrix}^\\top \\mathbf{x}_{k,t-1},\n",
    "$$\n",
    "$$\n",
    "K_{\\mathbf{\\eta}_{n,t-1}}(w_n) \n",
    "= \n",
    "\\begin{bmatrix} \n",
    "\\exp{ \\left ( -\\frac{\\beta}{1 + \\exp({-c_n^\\top b_S)}} \\mathbf{\\eta}_{n,t-1} - \\epsilon \\right ) }\n",
    "&\n",
    "1 - \\exp{ \\left ( -\\frac{\\beta}{1 + \\exp({-c_n^\\top b_S)}}\\mathbf{\\eta}_{n,t-1} - \\epsilon \\right ) }\n",
    "\\\\ \n",
    "1 - \\exp{ \\left ( -\\frac{1}{1 + \\exp{(-c_n^\\top b_R)}} \\right )}\n",
    "&\n",
    "\\exp{ \\left ( -\\frac{1}{1 + \\exp{(-c_n^\\top b_R)}} \\right )}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "$$\n",
    "    G(w_n) \n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    1 - q_S \n",
    "    & \n",
    "    q_S q_{Se}  \n",
    "    & \n",
    "    q_S (1-q_{Se})\\\\\n",
    "    1 - q_I \n",
    "    & \n",
    "    q_I (1- q_{Sp}) \n",
    "    & \n",
    "    q_I q_{Sp}\n",
    "    \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"prior_infection\":tf.convert_to_tensor([1-0.01, 0.01], dtype = tf.float32),\n",
    "              \"beta_l\":tf.convert_to_tensor([-2.0, +2.0], dtype = tf.float32),\n",
    "              \"beta_g\":tf.convert_to_tensor([-1.0, -1.0], dtype = tf.float32),\n",
    "              \"log_phi\":tf.math.log(\n",
    "                tf.convert_to_tensor([2.0], dtype = tf.float32)),\n",
    "              \"log_chi\":tf.math.log(\n",
    "                tf.convert_to_tensor([50.0], dtype = tf.float32)),\n",
    "              \"logit_sensitivity\":logit(\n",
    "                tf.convert_to_tensor([0.9], dtype = tf.float32)),\n",
    "              \"logit_specificity\":logit(\n",
    "                tf.convert_to_tensor([0.95], dtype = tf.float32)),\n",
    "              \"logit_prob_testing\":logit(\n",
    "                tf.convert_to_tensor([0.2, 0.5], dtype = tf.float32)),\n",
    "\t\t\"log_epsilon\":tf.math.log(tf.convert_to_tensor([0.001], dtype = tf.float32)),}\n",
    "\n",
    "T = 200\n",
    "n_covergage = 100\n",
    "n_gradient_steps = 1000\n",
    "n_trial = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pop = 1000\n",
    "covariates = tf.convert_to_tensor(np.load(\"Data/SpatialInference/Input/covariates.npy\"), dtype = tf.float32)[:N_pop,:]\n",
    "locations  = tf.convert_to_tensor(np.load(\"Data/SpatialInference/Input/locations.npy\"), dtype = tf.float32)[:N_pop,:]\n",
    "centroids  = tf.convert_to_tensor(np.load(\"Data/SpatialInference/Input/centroids.npy\"), dtype = tf.float32)\n",
    "cities_std = tf.convert_to_tensor(np.load(\"Data/SpatialInference/Input/cities_std.npy\"), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIS = spatial_SIS(locations, covariates)\n",
    "\n",
    "start = time.time()\n",
    "X, Y = simulator(SIS, parameters, T)\n",
    "print(time.time()-start)\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize = (10, 5), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(2):\n",
    "\tfor j in range(4):\n",
    "\n",
    "\t\tt = 10*(i*4+j)\n",
    "\n",
    "\t\tinfected = tf.where(X[t,0,:,1]==1)[:,0]\n",
    "\n",
    "\t\tax[i, j].set_title(\"Time \"+str(t))\n",
    "\t\tax[i, j].scatter(locations[:,0], locations[:,1], color = \"green\", s = 20)\n",
    "\t\tax[i, j].scatter(tf.gather(locations[:,0], infected),\n",
    "\t\t\t\t\ttf.gather(locations[:,1], infected), color = \"red\", s = 20)\n",
    "\t\tax[i, j].scatter(centroids[:,0], centroids[:,1], marker=\"*\", color = \"black\", s = 10)\n",
    "\n",
    "\t\tif j==0:\n",
    "\t\t\tfor city in range(tf.shape(cities_std)[0]):\n",
    "\t\t\t\tconfidence_ellipse(tf.linalg.diag(cities_std[city,:]), centroids[city,:], ax[i, j], edgecolor='black')\n",
    "\n",
    "plt.savefig('Figures/spatial_illustration.png', format='png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_parameters = { \"beta_l\": 2, \n",
    "\t\t\"beta_g\": 2, \n",
    "\t\t\"log_phi\": 1, \n",
    "\t\t\"log_chi\": 1, \n",
    "\t\t\"logit_prob_testing\": 2\n",
    "\t\t}\n",
    "\n",
    "\n",
    "ylabel_dict = { \"beta_l\" : \"$\\\\beta_{I}$\", \n",
    "\t\t\"beta_g\" : \"$\\\\beta_{R}$\", \n",
    "\t\t\"log_phi\": \"$\\\\log(\\\\phi)$\", \n",
    "\t\t\"log_chi\": \"$\\\\log(\\\\chi)$\", \n",
    "\t\t\"logit_prob_testing\": \"$logit(q_T)$\"\n",
    "\t\t}\n",
    "\n",
    "colors_list         = [\"blue\", \"black\", \"yellow\"]\n",
    "\n",
    "N_list = [500, 1000, 2000]\n",
    "\n",
    "fig, ax = plt.subplots(len(learning_parameters)+1, len(N_list), figsize = (len(N_list)*4, (len(learning_parameters)+1)*2), sharex=True)\n",
    "\n",
    "# ax[0,0].set_ylabel(\"normalized loss\")\n",
    "# ax[1,0].set_ylabel(r\"$\\log(\\phi)$\")\n",
    "# ax[2,0].set_ylabel(r\"$\\beta_{\\gamma}$\")\n",
    "\n",
    "# ax[len(learning_parameters),0].set_xlabel(\"iterations\")\n",
    "# ax[len(learning_parameters),1].set_xlabel(\"iterations\")\n",
    "# ax[len(learning_parameters),2].set_xlabel(\"iterations\")\n",
    "\n",
    "for n_index in range(len(N_list)):\n",
    "\tN = N_list[n_index]\n",
    "\n",
    "\tax[0,n_index].set_title(r\"$N=$\"+str(N))\n",
    "\n",
    "\toutput_path = \"Data/SpatialInference/Output/\"+str(N)+\"/\"\n",
    "\ttrue_loss_file_name        = \"spatial_inference_\"+str(N)+\"_true_loss.npy\"\n",
    "\toptim_loss_file_name       = \"spatial_inference_\"+str(N)+\"_optim_loss.npy\"\n",
    "\toptim_parameters_file_name = \"spatial_inference_\"+str(N)+\"_optim_parameters.npy\"\n",
    "\n",
    "\ttrue_loss        = np.load(output_path+true_loss_file_name,        allow_pickle=False)\n",
    "\toptim_loss       = np.load(output_path+optim_loss_file_name,       allow_pickle=False)\n",
    "\toptim_parameters = np.load(output_path+optim_parameters_file_name, allow_pickle=True)[()]\n",
    "\n",
    "\tindex = np.unique(np.where(np.all(optim_loss!=0, axis = -1))[0])\n",
    "\n",
    "\tfor i in range(n_covergage):\n",
    "\n",
    "\t\tif i in index:\n",
    "\n",
    "\t\t\ttrial_index = np.nanargmin(optim_loss[i,:,-1])\n",
    "\n",
    "\t\t\tax[0, n_index].plot(optim_loss[i,trial_index,:]/true_loss[i], color = \"blue\", alpha = 0.1)\n",
    "\t\t\tax[0, n_index].axhline(true_loss[i]/true_loss[i], color = \"red\", alpha = 0.1)\n",
    "\t\t\t\n",
    "\t\t\tif n_index == 0:\n",
    "\t\t\t\tax[0, n_index].set_ylabel(\"loss\")\t\n",
    "\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tfor key in learning_parameters.keys():\n",
    "\t\t\t\tfor j in range(learning_parameters[key]):\n",
    "\n",
    "\t\t\t\t\tax[counter+1, n_index].plot(optim_parameters[key][i,trial_index,:, j], color = colors_list[j], alpha = 0.1)\n",
    "\t\t\t\t\tax[counter+1, n_index].axhline(parameters[key][j].numpy(), color = \"red\", alpha = 0.1)\n",
    "\n",
    "\t\t\t\tif n_index == 0:\n",
    "\t\t\t\t\tax[counter+1, n_index].set_ylabel(ylabel_dict[key])\t\n",
    "\n",
    "\t\t\t\tif (counter+1) == len(learning_parameters.keys()):\n",
    "\t\t\t\t\tax[counter+1, n_index].set_xlabel(\"grad steps\")\n",
    "\t\t\t\t\n",
    "\t\t\t\tcounter = counter +1\n",
    "\n",
    "plt.savefig('Figures/spatial_inference.png', format='png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, len(learning_parameters), figsize = ((len(learning_parameters))*4, 2*2))\n",
    "\n",
    "table_dict = {}\n",
    "counter = 0\n",
    "for key in learning_parameters.keys():\n",
    "\n",
    "\ttable_dict[key] = []\n",
    "\n",
    "\tfor j in range(learning_parameters[key]):\n",
    "\t\toptim_param_list = []\n",
    "\t\tfor n_index in range(len(N_list)):\n",
    "\t\t\tN = N_list[n_index]\n",
    "\n",
    "\t\t\toutput_path = \"Data/SpatialInference/Output/\"+str(N)+\"/\"\n",
    "\t\t\ttrue_loss_file_name        = \"spatial_inference_\"+str(N)+\"_true_loss.npy\"\n",
    "\t\t\toptim_loss_file_name       = \"spatial_inference_\"+str(N)+\"_optim_loss.npy\"\n",
    "\t\t\toptim_parameters_file_name = \"spatial_inference_\"+str(N)+\"_optim_parameters.npy\"\n",
    "\n",
    "\t\t\ttrue_loss        = np.load(output_path+true_loss_file_name,        allow_pickle=False)\n",
    "\t\t\toptim_loss       = np.load(output_path+optim_loss_file_name,       allow_pickle=False)\n",
    "\t\t\toptim_parameters = np.load(output_path+optim_parameters_file_name, allow_pickle=True)[()]\n",
    "\n",
    "\t\t\tindex = np.unique(np.where(np.all(optim_loss!=0, axis = -1))[0])\n",
    "\n",
    "\t\t\ttrial_index = np.nanargmin(optim_loss[index,...,-1], axis =1)\n",
    "\t\t\tto_append = np.stack([optim_parameters[key][trial,trial_index[trial],-1,j] for trial in index])\n",
    "\t\t\t# to_append = optim_parameters[key][2,:,-1,j]\n",
    "\n",
    "\t\t\toptim_param_list.append(tf.gather(to_append, tf.where(tf.math.is_nan(to_append)==False))[:,0])\n",
    "\n",
    "\t\ttable_dict[key].append(optim_param_list)\n",
    "\t\t# ax[j,counter].boxplot(optim_param_list, showmeans=True)\n",
    "\t\t# ax[j,counter].axhline(parameters[key][j].numpy(), color = \"red\", alpha = 0.1)\n",
    "\n",
    "\tcounter = counter +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign data\n",
    "table = []\n",
    "\n",
    "for key in learning_parameters.keys():\n",
    "\tfor j in range(learning_parameters[key]):\n",
    "\t\trow = []\n",
    "\t\trow.append(ylabel_dict[key]+\" = \"+str(np.round(parameters[key][j].numpy(), 3)))\n",
    "\t\tfor n_index in range(len(N_list)):\n",
    "\n",
    "\t\t\trow.append(str(np.round(np.nanmean(table_dict[key][j][n_index]), 3))+\"+/-\"+str(np.round(np.nanstd(table_dict[key][j][n_index]), 3)))\n",
    "\t\t\n",
    "\t\ttable.append(row)\n",
    " \n",
    "# create header\n",
    "head = [\"Parameter\", \"N=100\", \"N=400\", \"N=2000\"]\n",
    " \n",
    "# display table\n",
    "print(tabulate(table, headers=head, tablefmt=\"latex_raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now an individual-based model with individual-specific covariates $w_n$ given by $(l_n, c_n)$ where $c_n$ is as in Section \\ref{sec:exp_spatial_model} while $l_n \\in [5]$ is a number between $1$ and $5$ representing the community the individual is assigned to, and we denote with $C_i$ the number of individuals in community $i$, i.e. $C_i=\\sum_{n \\in [N]}\\mathbb{I}(l_n=i)$. Specifically, we draw the probabilities of being assign to the different communities from a Dirichlet distribution with concentration parameters that are drawn from a folded Normal with mean $10$ and standard deviation $5$.\n",
    "\n",
    "Then we consider the following individual-specific initial distribution, transition kernel and emission distribution:\n",
    "$$\n",
    "    p_0(w_n,\\theta) \n",
    "    = \n",
    "    \\begin{bmatrix} \n",
    "    1-p_{0}\\\\ p_{0} \n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "$$\n",
    "    \\eta_{t-1}^N(w_n,\\theta)\n",
    "    =\n",
    "    \\frac{1}{N} \\sum_{\\tilde{n} \\in [N]\\setminus n} \\begin{bmatrix}\n",
    "    0\\\\\n",
    "    \\frac{N}{C_{l_{\\tilde{n}}}} B(l_n,l_{\\tilde{n}},\\phi)\n",
    "    \\end{bmatrix}^\\top x_{t-1}^N(w_{\\tilde{n}}),\n",
    "$$\n",
    "$$\n",
    "    K_{\\eta_{t-1}^N(w_n,\\theta)}(w_n,\\theta) \n",
    "    = \n",
    "    \\begin{bmatrix} \n",
    "    1 - \\frac{\\eta_{t-1}^N(w_n,\\theta) +\\epsilon }{1 + \\exp{(-\\beta_{I}^\\top c_n)}}\n",
    "    & \n",
    "    \\frac{\\eta_{t-1}^N(w_n,\\theta) +\\epsilon }{1 + \\exp{(-\\beta_{I}^\\top c_n)}}\\\\ \n",
    "    \\frac{1}{1 + \\exp{(-\\beta_{R}^\\top c_n)}}\n",
    "    & \n",
    "    1- \\frac{1}{1 + \\exp{(-\\beta_{R}^\\top c_n)}}\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "$$\n",
    "    G(w_n,\\theta) \n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    1 - q_T^{(1)} \n",
    "    & \n",
    "    q_T^{(1)} q_{Se}  \n",
    "    & \n",
    "    q_T^{(1)} (1-q_{Se})\\\\\n",
    "    1 - q_T^{(2)} \n",
    "    & \n",
    "    q_T^{(2)} (1- q_{Sp}) \n",
    "    & \n",
    "    q_T^{(2)} q_{Sp}\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "where $p_0$ is the probability of being infected at the beginning of the epidemic, $B(i,j,\\phi)= \\prod_{k=1}^3 \\exp ( -10 k \\phi\\mathbb{I}(|{i-j}|=k) )$ is a function that tells us the probability of a contact between $i$ and $j$, $\\epsilon$ is an environmental effect, $\\beta_I$ are the logistic regression coefficients for the infectious rate, $\\beta_R$ are the logistic regression coefficients for the recovery rate, $q_T$ is the vector of reporting probabilities with $q_T^{(1)}$ being reported as a susceptible and $q_T^{(2)}$ as an infected, $q_{Se}$ and $q_{Sp}$ are the sensitivity and specificity of the test. Essentially at each time step each individual interacts with the others according to a stochastic block model with a probability matrix that is given by $(B(i,j,\\phi))_{i,j \\in [5]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"prior_infection\":tf.convert_to_tensor([1-0.01, 0.01], dtype = tf.float32),\n",
    "              \"beta_l\":tf.convert_to_tensor([-1.0, +2.0], dtype = tf.float32),\n",
    "              \"beta_g\":tf.convert_to_tensor([-1.0, -1.0], dtype = tf.float32),\n",
    "\t      \"log_graph\":tf.math.log(tf.convert_to_tensor([0.1], dtype = tf.float32)),\n",
    "              \"logit_sensitivity\":logit(\n",
    "                tf.convert_to_tensor([0.9], dtype = tf.float32)),\n",
    "              \"logit_specificity\":logit(\n",
    "                tf.convert_to_tensor([0.95], dtype = tf.float32)),\n",
    "              \"logit_prob_testing\":logit(\n",
    "                tf.convert_to_tensor([0.2, 0.5], dtype = tf.float32)),\n",
    "\t      \"log_epsilon\":tf.math.log(tf.convert_to_tensor([0.001], dtype = tf.float32)),}\n",
    "\n",
    "\n",
    "T = 200\n",
    "n_covergage = 100\n",
    "n_gradient_steps = 1000\n",
    "n_trial = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pop = 10000\n",
    "covariates  = tf.convert_to_tensor(np.load(\"Data/GraphInference/Input/covariates.npy\"), dtype = tf.float32)[:N_pop,:]\n",
    "communities = tf.convert_to_tensor(np.load(\"Data/GraphInference/Input/communities.npy\"), dtype = tf.float32)[:N_pop,:]\n",
    "\n",
    "SIS = sbm_SIS(communities, covariates)\n",
    "\n",
    "start = time.time()\n",
    "X, Y = simulator(SIS, parameters, T)\n",
    "print(time.time()-start)\n",
    "X, Y = X[:,0,...], Y[:,0,...]\n",
    "\n",
    "B = SIS.B_matrix(parameters)\n",
    "nodes = range(B.shape[0])\n",
    "G = nx.Graph()\n",
    "for i in nodes:\n",
    "    for j in nodes:\n",
    "        if i > j and B.numpy()[i, j]!=0:\n",
    "            G.add_edge(i, j, weight=10*B.numpy()[i, j])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (2, 2))      \n",
    "ax.imshow(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = tf.reduce_sum(X, axis = 1)\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize = (15, 7), sharex=True, sharey=True)\n",
    "\n",
    "jump_list = [5, 10]\n",
    "for i in range(2):\n",
    "\tfor j in range(4):\n",
    "\n",
    "\t\tt = jump_list[i]*(i*4+j)\n",
    "\n",
    "\t\tax[i, j].set_title(\"Time \"+str(t))\n",
    "\t\t\n",
    "\t\tcurrent_state = tf.einsum(\"tns,nc->tcs\", X, SIS.communities)[t,...]\n",
    "\t\tpercentage    = current_state/tf.reduce_sum(current_state, axis = 1, keepdims=True)\n",
    "\n",
    "\t\tpos = nx.spring_layout(G, seed = 10)  # Positions for all nodes\n",
    "\t\tweight_labels = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "\t\tcolor_map = [\"green\" for node in G.nodes] \n",
    "\t\tnx.draw_networkx_nodes( G, pos, \n",
    "\t\t\t\t\tnode_color = color_map, node_shape='o', node_size = 0.25*tf.reduce_sum(SIS.communities, axis = 0),\n",
    "\t\t\t\t\tax = ax[i,j])\n",
    "\n",
    "\t\tcolor_map = [\"red\" for node in G.nodes] \n",
    "\t\tnx.draw_networkx_nodes( G, pos, \n",
    "\t\t\t \t\tnode_color=color_map, node_shape='o', node_size = 0.25*tf.reduce_sum(SIS.communities, axis = 0)*percentage[:,1],\n",
    "\t\t\t\t\tax = ax[i,j])\n",
    "\t\t\n",
    "\t\tnx.draw_networkx_edges( G, pos, \n",
    "\t\t\t \t\tedgelist = weight_labels.keys(), width=list(weight_labels.values()), edge_color='black', alpha=0.5,\n",
    "\t\t\t\t\tax = ax[i,j])\n",
    "\t\t\n",
    "plt.savefig('Figures/graph_illustration.png', format='png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_parameters = { \"beta_l\": 2, \n",
    "\t\t\t\"beta_g\": 2, \n",
    "\t\t\t\"logit_prob_testing\": 2, \n",
    "\t\t\t\"log_graph\": 1}\n",
    "\n",
    "ylabel_dict = { \"beta_l\": \"$\\\\beta_{I}$\", \n",
    "\t\t\"beta_g\": \"$\\\\beta_{R}$\", \n",
    "\t\t\"logit_prob_testing\": \"$logit(q_T)$\", \n",
    "\t\t\"log_graph\": \"$\\\\log(\\\\phi)$\"}\n",
    "\n",
    "colors_list         = [\"blue\", \"black\", \"yellow\"]\n",
    "\n",
    "N_list = [500, 5000, 50000]\n",
    "\n",
    "fig, ax = plt.subplots(len(learning_parameters)+1, len(N_list), figsize = (len(N_list)*4, (len(learning_parameters)+1)*2), sharex=True)\n",
    "\n",
    "# ax[0,0].set_ylabel(\"normalized loss\")\n",
    "# ax[1,0].set_ylabel(r\"$\\log(\\phi)$\")\n",
    "# ax[2,0].set_ylabel(r\"$\\beta_{\\gamma}$\")\n",
    "\n",
    "# ax[len(learning_parameters),0].set_xlabel(\"iterations\")\n",
    "# ax[len(learning_parameters),1].set_xlabel(\"iterations\")\n",
    "# ax[len(learning_parameters),2].set_xlabel(\"iterations\")\n",
    "\n",
    "for n_index in range(len(N_list)):\n",
    "\tN = N_list[n_index]\n",
    "\n",
    "\tax[0,n_index].set_title(r\"$N=$\"+str(N))\n",
    "\n",
    "\toutput_path = \"Data/GraphInference/Output/\"+str(N)+\"/\"\n",
    "\ttrue_loss_file_name        = \"graph_inference_\"+str(N)+\"_true_loss.npy\"\n",
    "\toptim_loss_file_name       = \"graph_inference_\"+str(N)+\"_optim_loss.npy\"\n",
    "\toptim_parameters_file_name = \"graph_inference_\"+str(N)+\"_optim_parameters.npy\"\n",
    "\n",
    "\ttrue_loss        = np.load(output_path+true_loss_file_name,        allow_pickle=False)\n",
    "\toptim_loss       = np.load(output_path+optim_loss_file_name,       allow_pickle=False)\n",
    "\toptim_parameters = np.load(output_path+optim_parameters_file_name, allow_pickle=True)[()]\n",
    "\n",
    "\tindex = np.unique(np.where(np.all(optim_loss!=0, axis = -1))[0])\n",
    "\n",
    "\tfor i in range(n_covergage):\n",
    "\n",
    "\t\tif i in index:\n",
    "\n",
    "\t\t\ttrial_index = np.nanargmin(optim_loss[i,:,-1])\n",
    "\n",
    "\t\t\tax[0, n_index].plot(optim_loss[i,trial_index,:]/true_loss[i], color = \"blue\", alpha = 0.1)\n",
    "\t\t\tax[0, n_index].axhline(true_loss[i]/true_loss[i], color = \"red\", alpha = 0.1)\n",
    "\t\t\tif n_index == 0:\n",
    "\t\t\t\tax[0, n_index].set_ylabel(\"loss\")\t\n",
    "\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tfor key in learning_parameters.keys():\n",
    "\t\t\t\tfor j in range(learning_parameters[key]):\n",
    "\n",
    "\t\t\t\t\tax[counter+1, n_index].plot(optim_parameters[key][i,trial_index,:, j], color = colors_list[j], alpha = 0.1)\n",
    "\t\t\t\t\tax[counter+1, n_index].axhline(parameters[key][j].numpy(), color = \"red\", alpha = 0.1)\n",
    "\n",
    "\t\t\t\tif n_index == 0:\n",
    "\t\t\t\t\tax[counter+1, n_index].set_ylabel(ylabel_dict[key])\t\n",
    "\n",
    "\t\t\t\tif (counter+1) == len(learning_parameters.keys()):\n",
    "\t\t\t\t\tax[counter+1, n_index].set_xlabel(\"grad steps\")\n",
    "\n",
    "\t\t\t\tcounter = counter +1\n",
    "\n",
    "\tplt.savefig('Figures/graph_inference.png', format='png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, len(learning_parameters), figsize = ((len(learning_parameters))*5, 10))\n",
    "\n",
    "table_dict = {}\n",
    "counter = 0\n",
    "for key in learning_parameters.keys():\n",
    "\ttable_dict[key] = []\n",
    "\tfor j in range(learning_parameters[key]):\n",
    "\t\toptim_param_list = []\n",
    "\t\tfor n_index in range(len(N_list)):\n",
    "\t\t\tN = N_list[n_index]\n",
    "\n",
    "\t\t\toutput_path = \"Data/GraphInference/Output/\"+str(N)+\"/\"\n",
    "\t\t\ttrue_loss_file_name        = \"graph_inference_\"+str(N)+\"_true_loss.npy\"\n",
    "\t\t\toptim_loss_file_name       = \"graph_inference_\"+str(N)+\"_optim_loss.npy\"\n",
    "\t\t\toptim_parameters_file_name = \"graph_inference_\"+str(N)+\"_optim_parameters.npy\"\n",
    "\n",
    "\t\t\ttrue_loss        = np.load(output_path+true_loss_file_name,        allow_pickle=False)\n",
    "\t\t\toptim_loss       = np.load(output_path+optim_loss_file_name,       allow_pickle=False)\n",
    "\t\t\toptim_parameters = np.load(output_path+optim_parameters_file_name, allow_pickle=True)[()]\n",
    "\n",
    "\t\t\tindex = np.unique(np.where(np.all(optim_loss!=0, axis = -1))[0])\n",
    "\n",
    "\t\t\ttrial_index = np.nanargmin(optim_loss[index,...,-1], axis =1)\n",
    "\t\t\tto_append = np.stack([optim_parameters[key][trial,trial_index[trial],-1,j] for trial in index])\n",
    "\t\t\t# to_append = optim_parameters[key][2,:,-1,j]\n",
    "\n",
    "\t\t\toptim_param_list.append(tf.gather(to_append, tf.where(tf.math.is_nan(to_append)==False))[:,0])\n",
    "\n",
    "\t\ttable_dict[key].append(optim_param_list)\n",
    "\t\t# ax[j,counter].boxplot(optim_param_list, showmeans=True)\n",
    "\t\t# ax[j,counter].axhline(parameters[key][j].numpy(), color = \"red\", alpha = 0.1)\n",
    "\n",
    "\tcounter = counter +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign data\n",
    "table = []\n",
    "\n",
    "for key in learning_parameters.keys():\n",
    "\tfor j in range(learning_parameters[key]):\n",
    "\t\trow = []\n",
    "\t\trow.append(ylabel_dict[key]+\" \"+str(np.round(parameters[key][j].numpy(), 3)))\n",
    "\t\tfor n_index in range(len(N_list)):\n",
    "\n",
    "\t\t\trow.append(str(np.round(np.nanmean(table_dict[key][j][n_index]), 4))+\"+/-\"+str(np.round(np.nanstd(table_dict[key][j][n_index]), 4)))\n",
    "\t\n",
    "\t\ttable.append(row)\n",
    " \n",
    "# create header\n",
    "head = [\"Parameter\", \"N=500\", \"N=5000\", \"N=50000\"]\n",
    " \n",
    "# display table\n",
    "print(tabulate(table, headers=head, tablefmt=\"latex_raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_parameters = {#\"logit_prior_infection\":18053, \n",
    "\t\t       \"log_tau\":1,\n",
    "\t\t       \"log_delta\":1, \"log_zeta\":1, \"log_xi\":1, \n",
    "\t\t       \"log_chi\":1, \"log_psi\":1, \"log_gamma\":1,\"log_epsilon\":1}\n",
    "\t\n",
    "\n",
    "ylabel_dict = {#\"logit_prior_infection\": r\"$\\text{logit}(\\pi_{n,0})$\", \n",
    "\t\t\"log_tau\": r\"$\\log(\\tau)$\", \n",
    "\t\t\"log_delta\":r\"$\\log(\\delta)$\", \"log_zeta\":r\"$\\log(\\zeta)$\", \"log_xi\":r\"$\\log(\\xi)$\", \n",
    "\t\t\"log_chi\":r\"$\\log(\\chi)$\", \"log_psi\":r\"$\\log(\\psi)$\", \"log_gamma\":r\"$\\log(\\gamma)$\",\"log_epsilon\":r\"$\\log(\\epsilon)$\"}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(int(len(learning_parameters.keys())/2), 2, figsize = (5*2, 2.5*int(len(learning_parameters.keys())/2)), sharex=True)\n",
    "\n",
    "list_best_param = []\n",
    "list_best_loss = []\n",
    "\n",
    "best_initial = []\n",
    "\n",
    "for test in range(10):\n",
    "\toutput_path = \"Data/FM/current/Output_\"+str(test)+\"/\"\n",
    "\tloss_file_name       = \"FM_loss.npy\"\n",
    "\n",
    "\tloss       = np.load(output_path+loss_file_name,       allow_pickle=False)\n",
    "\tbest_index = np.argmin(loss[:,-1])\n",
    "\n",
    "\tlist_best_loss.append(loss[best_index,:])\n",
    "\n",
    "\tdict_best_param = {}\n",
    "\n",
    "\tfor i in range(int(len(learning_parameters.keys())/2)):\n",
    "\t\tfor j in range(2):\n",
    "\n",
    "\t\t\tkey = list(learning_parameters.keys())[2*i+j]\n",
    "\n",
    "\t\t\toptim_parameters_file_name = \"FM_parameters_\"+key+\".npy\"\n",
    "\t\t\toptim_parameters = np.load(output_path+optim_parameters_file_name)[...,0]\n",
    "\n",
    "\t\t\tdict_best_param[key] = optim_parameters[-1,-1]\n",
    "\n",
    "\t\t\tax[i,j].plot(optim_parameters[best_index,:], color = \"blue\")\n",
    "\n",
    "\t\t\tax[i,j].set_ylabel(ylabel_dict[list(learning_parameters.keys())[2*i+j]])\t\n",
    "\n",
    "\tkey = \"log_epsilon\"#\n",
    "\toptim_parameters_file_name = \"FM_parameters_\"+key+\".npy\"\n",
    "\toptim_parameters = np.load(output_path+optim_parameters_file_name)[...,0]\n",
    "\tdict_best_param[key] = optim_parameters[-1,-1]\n",
    "\n",
    "\tlist_best_param.append(dict_best_param)\n",
    "\n",
    "\t# plt.savefig('Figures/graph_inference.png', format='png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sample = np.stack(list_best_loss)\n",
    "\n",
    "parameters_list_sample = {}\n",
    "for key in list_best_param[0].keys():\n",
    "\tparameters_list_sample[key] = []\n",
    "\n",
    "\tfor i in range(len(list_best_param)):\n",
    "\t\tparameters_list_sample[key].append(list_best_param[i][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_sample = {}\n",
    "\n",
    "for key in list_best_param[0].keys():\n",
    "\t\n",
    "\tparameters_sample[key] = np.stack(parameters_list_sample[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sim = np.argmin(loss_sample[:,-1])\n",
    "\n",
    "best_parameters = {}\n",
    "\n",
    "for key in list_best_param[0].keys():\n",
    "\tbest_parameters[key] = parameters_sample[key][best_sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"Scripts/\")\n",
    "%run -i FM_model.py\n",
    "%run -i CAL.py\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.convert_to_tensor(np.load(\"Data/FM/Y_FM_cumbria.npy\"), dtype = tf.float32)\n",
    "indexes = tf.convert_to_tensor(np.load(\"Data/FM/indexes_FM_cumbria.npy\"), dtype = tf.int64)\n",
    "values  = tf.convert_to_tensor(np.load(\"Data/FM/values_FM_cumbria.npy\"), dtype = tf.float32)\n",
    "covariates = tf.convert_to_tensor(np.load(\"Data/FM/covariates_FM_cumbria.npy\"), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "parameters = {}\n",
    "for key in best_parameters.keys():\n",
    "\tparameters[key] = tf.convert_to_tensor(best_parameters[key], dtype = tf.float32) \n",
    "parameters[\"logit_prob_testing\"] = logit(tf.convert_to_tensor([0.0, 0.0, 1.0, 0.0], dtype = tf.float32))\n",
    "\n",
    "FM_ibm = sparse_FM_SINR(values, indexes, covariates)\n",
    "\n",
    "start = time.time()\n",
    "loss = CAL_loss(FM_ibm, parameters, Y[:100])\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_list = []\n",
    "# for i in range(100):\n",
    "# \tprint(i)\n",
    "# \tX, Y = FM_simulator(FM_ibm, parameters, 200)\n",
    "# \tY_list.append(tf.reduce_sum(Y, axis = 1)[:,3])\n",
    "\n",
    "# np.save(\"Data/FM/Y_FM_list_stack.npy\", np.stack(Y_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_index = np.argmin(loss_sample[:,-1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "ax[0].plot(loss_sample[best_loss_index,:])\n",
    "ax[0].set_xlabel(\"iteration\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "\n",
    "\n",
    "Y_cumbria = tf.convert_to_tensor(np.load(\"Data/FM/Y_FM_cumbria.npy\"), dtype = tf.float32)\n",
    "Y_list_stack = np.load(\"Data/FM/Y_FM_list_stack.npy\")\n",
    "\n",
    "time = np.linspace(1, 200, 200)\n",
    "ax[2].plot(time, np.mean(Y_list_stack, axis = 0), color = \"blue\", )\n",
    "ax[2].fill_between(time, np.quantile(Y_list_stack, (0.025, 0.975), axis = 0)[0,:], np.quantile(Y_list_stack, (0.05, 0.95), axis = 0)[1,:], color = \"blue\", alpha = 0.2)\n",
    "ax[2].plot(time, tf.reduce_sum(Y_cumbria, axis = 1)[:200, 3], color = \"blue\", linestyle='dashed')\n",
    "ax[2].set_xlabel(\"time\")\n",
    "ax[2].set_ylabel(\"notified\")\n",
    "\n",
    "Pi, Mu, Log_likelihood = CAL(FM_ibm, parameters, Y_cumbria)\n",
    "\n",
    "ax[1].plot(time, tf.reduce_sum(Y_cumbria, axis = 1)[:200, 3], color = \"blue\", linestyle='dashed')\n",
    "ax[1].plot(time, tf.reduce_sum(Mu, axis = 1)[:200,3], color = \"blue\", )\n",
    "\n",
    "lower = tf.reduce_sum(Mu, axis = 1)[:200,3] - 1.96*tf.reduce_sum(Mu*(1-Mu), axis = 1)[:200,3] \n",
    "lower = tf.reduce_max(tf.stack((lower, tf.zeros(tf.shape(lower))), axis = -1), axis = -1)\n",
    "upper = tf.reduce_sum(Mu, axis = 1)[:200,3] + 1.96*tf.reduce_sum(Mu*(1-Mu), axis = 1)[:200,3]\n",
    "ax[1].fill_between(time, lower, upper, color = \"blue\", alpha=0.1)\n",
    "ax[1].set_xlabel(\"time\")\n",
    "ax[1].set_ylabel(\"notified\")\n",
    "\n",
    "plt.savefig('Figures/FM_predictive.png', format='png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign data\n",
    "# table = []\n",
    "\n",
    "# farm = [[100, 0], [0, 1000], [50, 500], [2, 6]]\n",
    "\n",
    "# for i in range(4):\n",
    "# \trow = []\n",
    "# \trow.append(farm[i][0])\n",
    "# \trow.append(farm[i][1])\n",
    "# \trow.append(np.exp(parameters[\"log_xi\"])*(farm[i][0]**(np.exp(parameters[\"log_chi\"]))) + (farm[i][1]**(np.exp(parameters[\"log_chi\"]))))\n",
    "# \trow.append(np.exp(parameters[\"log_zeta\"])*(farm[i][0]**(np.exp(parameters[\"log_chi\"]))) + (farm[i][1]**(np.exp(parameters[\"log_chi\"]))))\n",
    "# \ttable.append(row)\n",
    "\n",
    "# # create header\n",
    "# head = [\"Nr. cattle\", \"Nr. sheep\", \"Mean susceptibility\", \"Mean infectivity\"]\n",
    " \n",
    "# # display table\n",
    "# print(tabulate(table, headers=head, tablefmt=\"latex_raw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def f(x, y):\n",
    "    return np.exp(parameters[\"log_xi\"])*(x**(np.exp(parameters[\"log_chi\"]))) + (y**(np.exp(parameters[\"log_chi\"])))\n",
    "\n",
    "def h(x, y):\n",
    "    return np.exp(parameters[\"log_zeta\"])*(x**(np.exp(parameters[\"log_chi\"]))) + (y**(np.exp(parameters[\"log_chi\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 200, 100)\n",
    "y = np.linspace(0, 2000, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z1 = f(X, Y)\n",
    "Z2 = h(X, Y)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "sns.heatmap(Z1, ax=ax1, cmap='viridis', origin = \"lower\")\n",
    "ax1.set_title('susceptibility')\n",
    "ax1.set_xlabel('nr. cattle')  \n",
    "ax1.set_ylabel('nr. sheep')  \n",
    "\n",
    "# Set the x and y axis ticks to match x and y values\n",
    "ax1.set_xticks(np.linspace(0, len(x)-1, num=5))  # 5 evenly spaced ticks\n",
    "ax1.set_xticklabels(np.linspace(x.min(), x.max(), num=5).astype(int))\n",
    "ax1.set_yticks(np.linspace(0, len(y)-1, num=5))  # 5 evenly spaced ticks\n",
    "ax1.set_yticklabels(np.linspace(y.min(), y.max(), num=5).astype(int))\n",
    "\n",
    "sns.heatmap(Z2, ax=ax2, cmap='plasma', origin = \"lower\")\n",
    "ax2.set_title('infectivity')\n",
    "ax2.set_xlabel('nr. cattle')  \n",
    "ax2.set_ylabel('nr. sheep')  \n",
    "\n",
    "# Set the x and y axis ticks to match x and y values\n",
    "ax2.set_xticks(np.linspace(0, len(x)-1, num=5))  # 5 evenly spaced ticks\n",
    "ax2.set_xticklabels(np.linspace(x.min(), x.max(), num=5).astype(int))\n",
    "ax2.set_yticks(np.linspace(0, len(y)-1, num=5))  # 5 evenly spaced ticks\n",
    "ax2.set_yticklabels(np.linspace(y.min(), y.max(), num=5).astype(int))\n",
    "\n",
    "ax3.plot(np.linspace(0, 20, 21), np.exp(parameters[\"log_psi\"])/(np.exp(2*parameters[\"log_psi\"]) + np.linspace(0, 20, 21)*np.linspace(0, 20, 21))*np.where(np.linspace(0, 20, 21)>15, 0, 1))\n",
    "ax3.set_xticks(np.linspace(0, 20, num=5))\n",
    "ax3.set_xlabel(\"distance (Km)\")\n",
    "ax3.set_ylabel(\"spatial kernel\")\n",
    "fig.tight_layout()\n",
    "plt.savefig('Figures/FM_susc_inf_spat.png', format='png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfwsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
